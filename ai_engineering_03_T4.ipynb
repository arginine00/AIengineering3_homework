{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_6-jdBOXowG"
      },
      "source": [
        "**注意事項**\n",
        "\n",
        "このノートブックは、GPU:「T4」に対応させたものです。\n",
        "「L4」版のノートブックとはモデル等が異なるため、生成される内容が異なることが考えられます。\n",
        "\n",
        "生成される内容と、ノートブックに記載されている説明が一致しない場合があることをご了承ください。\n",
        "\n",
        "生成内容とノートブックの説明をよく見比べ、適宜読み替えながら演習を進めてみてください。\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4EN4GmtStsN"
      },
      "source": [
        "# 演習の方針\n",
        "\n",
        "1. **ベースラインモデル評価**  \n",
        "   素のモデルで回答を生成し、講義内容との整合性の低さを観察します。これにより、特別な学習なしでのモデルの限界を確認します。\n",
        "\n",
        "2. **文字起こしデータの活用**  \n",
        "   講義の文字起こしデータを導入し、モデルが講義内容を参照した回答を生成する傾向を観察します。ただし、Retrieval（情報検索）精度の限界から結果は不安定になる可能性があります。\n",
        "\n",
        "3. **チャンク化の導入**  \n",
        "   文字起こしデータをチャンク（小単位）に分割し、より安定して関連コンテンツを取得できるようにします。この段階では文脈理解にまだ課題があることを確認します。\n",
        "\n",
        "4. **Rerankの適用**  \n",
        "   検索結果のランク付けを導入し、より的確で安定した回答を目指します。\n",
        "\n",
        "5. **応用改善手法**  \n",
        "   文字起こしの品質向上のための編集技術や、メタデータの活用による性能向上手法を探ります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPI1pj4mFavt"
      },
      "source": [
        "## 扱う質問\n",
        "\n",
        "「Inference Time Scaling（推論時スケーリング）」に関する質問を取り扱います。これは以下の背景を持つトピックです。\n",
        "\n",
        "- 2024年8月発表の論文「Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters」で提唱された概念\n",
        "- OpenAIのGPT-o1（2024年9月リリース）で実用化され、注目を集めた比較的新しいアプローチ\n",
        "- 2024年度LLM講座の第4回講義でも取り上げられた重要テーマ\n",
        "\n",
        "## 扱うモデル\n",
        "\n",
        "「google/gemma-2-2b-jpn-it」を使用します。このモデルは、リリース時期の関係上、以下の特徴を持ちます。\n",
        "\n",
        "- 「Inference Time Scaling」の概念が広まる前に訓練されており、このトピックに関する知識を持たないと想定される\n",
        "- この特性を活かし、純粋なベースライン評価から各手法の効果を観察する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bla6WHyQStsO"
      },
      "source": [
        "### 演習環境の準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vM50WAI7GXwC",
        "outputId": "df59c60b-6c9f-4e2e-a1cf-fcc0b15f9a73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: google-colab-selenium in /usr/local/lib/python3.11/dist-packages (1.0.14)\n",
            "Requirement already satisfied: selenium in /usr/local/lib/python3.11/dist-packages (from google-colab-selenium) (4.32.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium->google-colab-selenium) (2.4.0)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (0.30.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (0.12.2)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (2025.4.26)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium->google-colab-selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium->google-colab-selenium) (1.3.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.11/dist-packages (from trio-websocket~=0.9->selenium->google-colab-selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium->google-colab-selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium->google-colab-selenium) (0.16.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install google-colab-selenium\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V2PStE0uqM03",
        "outputId": "0885fe6f-3679-46bb-ac3f-3ce35a4d8aba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'lecture-ai-engineering' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# 演習用のコンテンツを取得\n",
        "!git clone https://github.com/matsuolab/lecture-ai-engineering.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zXo_kFASXlvp",
        "outputId": "47a7340e-c84d-40ed-8e11-580d657a36ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "95fa6bb538884010bbc66884852df506",
            "1ea9b696446b4bd584ea886051b5d982",
            "3b3cab67461347808789838cfc744e1c",
            "db4b5a26d62943bb84f83bf6f272d18a",
            "32db503f37af44acbc6decbfa68c335e",
            "8e168a6efe504814a70326992590045c",
            "72e8723090d14f14b167e911599beec9",
            "18b43a21a8524ebb954092d8df3c630e",
            "ea2ae871a67441c1891c319ef6497ecc",
            "c157587fd8c0406a88255fc79de2e099",
            "9c525cb180c4425398a88aec3860f368",
            "cd8dfa238fd9450a94f3e663ad861d02",
            "fea17e9a044945d8bf91bd8d5cfbbd05",
            "23914dd39cb44ca4a77e4c5617842650",
            "5bf285b1cedb495a9776d73efba7007b",
            "b71e366ebdf941ad849bd1085e8b962d",
            "cdb2fd205a4545bea4eca89022e3738e",
            "1aafe1c212364895a8dac33bac0d7766",
            "263049ac05fd439f9dce4c84cc454022",
            "299e5ff12ff8494db022e3b4a359b833"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "95fa6bb538884010bbc66884852df506"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# HuggingFace Login\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dZ_NUIftXwLc"
      },
      "outputs": [],
      "source": [
        "# CUDAが利用可能ならGPUを、それ以外ならCPUをデバイスとして設定\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7eTgV8XBPA90"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6tV9mO8oXoaM",
        "outputId": "819d611e-b07a-427f-bf03-0d6282d5f068",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-8c2fb1933daf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4378\u001b[0m         \u001b[0;31m# Prepare the full device map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4380\u001b[0;31m             \u001b[0mdevice_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_in_fp32_regex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4382\u001b[0m         \u001b[0;31m# Finalize model weight initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_get_device_map\u001b[0;34m(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m             \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"disk\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    105\u001b[0m                     \u001b[0;34m\"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0;34m\"quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
          ]
        }
      ],
      "source": [
        "# モデル(Gemma2)の読み込み\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"google/gemma-2-2b-jpn-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=bnb_config,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piTdVxTfGcc_"
      },
      "source": [
        "# 1. ベースラインモデル評価\n",
        "**まずはベースモデルがどの程度知識を持っているか確かめる**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_output(query):\n",
        "  messages = [\n",
        "      {\"role\": \"user\", \"content\": query},\n",
        "  ]\n",
        "  input_ids = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      add_generation_prompt=True,\n",
        "      return_tensors=\"pt\"\n",
        "  ).to(model.device)\n",
        "\n",
        "  terminators = [\n",
        "      tokenizer.eos_token_id,\n",
        "      tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "  ]\n",
        "\n",
        "  outputs = model.generate(\n",
        "      input_ids,\n",
        "      max_new_tokens=256,\n",
        "      eos_token_id=terminators,\n",
        "      do_sample=False,\n",
        "      # temperature=0.6, # If do_sample=True\n",
        "      # top_p=0.9,  # If do_sample=True\n",
        "  )\n",
        "\n",
        "  response = outputs[0][input_ids.shape[-1]:]\n",
        "  return tokenizer.decode(response, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "huDNYbXW3lOm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NBUZ3o6dhMlC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f062811a-b1e3-4412-ad2b-037ed4d22ee0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "数学における双子素数は、**2つの素数**で構成される**2つの異なる素数のペア**です。 \n",
            "\n",
            "**具体的には:**\n",
            "\n",
            "* **双子素数**は、**2つの異なる素数**で構成される**2つの異なる素数のペア**です。\n",
            "* **素数**とは、1と自身以外に因数を持たない自然数のことです。\n",
            "\n",
            "\n",
            " \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "question =  \"数学における双子素数とは何ですか?\"\n",
        "response = generate_output(question)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 数値的な評価も見てみます。RagasにはAnswer Accuracyという評価指標があります。今回はこちらを参考に実装した評価関数を利用して測っていきます。\n",
        "\n",
        "- 今回はgemmaでは性能が不安定だったので、OpenAIのgpt-4oで評価していきます。従って、scoreの実行はopenAI APIキーを所持している関心がある方のみで良いです。"
      ],
      "metadata": {
        "id": "fG9zI6_lAYsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fncw05_I_IVl",
        "outputId": "c127167c-e7ed-4af8-c23a-c98bfcd15658",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.77.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 評価実装\n",
        "# gold_answer = \"「Inference Time Scaling」とは、推論時に計算量を増やしてモデルの性能を高める手法です。これはモデルのサイズを大きくする代わりに、難しい入力に対して多くの計算リソースを使うことで、より良い出力を得ようとするアプローチです。\"\n",
        "\n",
        "# from openai import OpenAI\n",
        "# from google.colab import userdata\n",
        "# client = OpenAI(api_key=userdata.get(\"OPENAI_API_KEY\"), max_retries=5, timeout=60)\n",
        "\n",
        "# def openai_generator(query):\n",
        "\n",
        "#         messages = [\n",
        "#                     {\n",
        "#                         \"role\": \"user\",\n",
        "#                         \"content\": query\n",
        "#                     }\n",
        "#                 ]\n",
        "\n",
        "#         response = client.chat.completions.create(\n",
        "#             model=\"gpt-4o-mini\",\n",
        "#             messages=messages\n",
        "#         )\n",
        "#         return response.choices[0].message.content\n",
        "\n",
        "# def evaluate_answer_accuracy(query, response, reference):\n",
        "\n",
        "#     template_accuracy1 = (\n",
        "#           \"Instruction: You are a world class state of the art assistant for rating \"\n",
        "#           \"a User Answer given a Question. The Question is completely answered by the Reference Answer.\\n\"\n",
        "#           \"Say 4, if User Answer is full contained and equivalent to Reference Answer\"\n",
        "#           \"in all terms, topics, numbers, metrics, dates and units.\\n\"\n",
        "#           \"Say 2, if User Answer is partially contained and almost equivalent to Reference Answer\"\n",
        "#           \"in all terms, topics, numbers, metrics, dates and units.\\n\"\n",
        "#           \"Say 0, if User Answer is not contained in Reference Answer or not accurate in all terms, topics,\"\n",
        "#           \"numbers, metrics, dates and units or the User Answer do not answer the question.\\n\"\n",
        "#           \"Do not explain or justify your rating. Your rating must be only 4, 2 or 0 according to the instructions above.\\n\"\n",
        "#           \"Even small discrepancies in meaning, terminology, directionality, or implication must result in a lower score. Only rate 4 if the User Answer is a complete and precise match to the Reference Answer in every aspect.\\n\"\n",
        "#           \"### Question: {query}\\n\"\n",
        "#           \"### {answer0}: {sentence_inference}\\n\"\n",
        "#           \"### {answer1}: {sentence_true}\\n\"\n",
        "#           \"The rating is:\\n\"\n",
        "#       )\n",
        "#     template_accuracy2 = (\n",
        "#           \"I will rate the User Answer in comparison to the Reference Answer for a given Question.\\n\"\n",
        "#           \"A rating of 4 indicates that the User Answer is entirely consistent with the Reference Answer, covering all aspects, topics, numbers, metrics, dates, and units.\\n\"\n",
        "#           \"A rating of 2 signifies that the User Answer is mostly aligned with the Reference Answer, with minor discrepancies in some areas.\\n\"\n",
        "#           \"A rating of 0 means that the User Answer is either inaccurate, incomplete, or unrelated to the Reference Answer, or it fails to address the Question.\\n\"\n",
        "#           \"I will provide the rating without any explanation or justification, adhering to the following scale: 0 (no match), 2 (partial match), 4 (exact match).\\n\"\n",
        "#           \"Even minor inconsistencies in meaning, terminology, emphasis, or factual detail should prevent a rating of 4. Only assign a 4 if the User Answer exactly and unambiguously matches the Reference Answer in every respect.\"\n",
        "#           \"Do not explain or justify my rating. My rating must be only 4, 2 or 0 only.\\n\\n\"\n",
        "#           \"Question: {query}\\n\\n\"\n",
        "#           \"{answer0}: {sentence_inference}\\n\\n\"\n",
        "#           \"{answer1}: {sentence_true}\\n\\n\"\n",
        "#           \"Rating: \"\n",
        "#       )\n",
        "\n",
        "#     score1 = openai_generator(\n",
        "#                 template_accuracy1.format(\n",
        "#                       query=query,\n",
        "#                       answer0=\"User Answer\",\n",
        "#                       answer1=\"Reference Answer\",\n",
        "#                       sentence_inference=response,\n",
        "#                       sentence_true=reference,\n",
        "#                     )\n",
        "#                 )\n",
        "#     try:\n",
        "#       score1 = int(score1)\n",
        "#     except:\n",
        "#       print(\"Failed\")\n",
        "#       score1 = 0\n",
        "\n",
        "#     score2 = openai_generator(\n",
        "#                 template_accuracy2.format(\n",
        "#                         query=query,\n",
        "#                         answer0=\"Reference Answer\",\n",
        "#                         answer1=\"User Answer\",\n",
        "#                         sentence_inference=reference,\n",
        "#                         sentence_true=response,\n",
        "#                     )\n",
        "#                   )\n",
        "\n",
        "#     try:\n",
        "#       score2 = int(score2)\n",
        "#     except:\n",
        "#       print(\"Failed\")\n",
        "#       score2 = 0\n",
        "\n",
        "\n",
        "#     return (score1 + score2) / 2"
      ],
      "metadata": {
        "id": "t89v938Y1o4I",
        "cellView": "form"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 評価\n",
        "# score = evaluate_answer_accuracy(question, response, gold_answer)\n",
        "# print(score)"
      ],
      "metadata": {
        "id": "CPLGyk7T5LaM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSCNnRf9pJif"
      },
      "source": [
        "## 結果 (ベースモデル)\n",
        "\n",
        "「google/gemma-2-2b-jpn-it」は「Inference Time Scaling」について誤った知識を提示しました：\n",
        "* モデルは従来の「推論時間の短縮」という文脈でInference Time Scalingを解釈しており、これはLLM分野における最新の「Inference Time Scaling」概念（推論時計算資源の最適配分）とは異なる説明になります。\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4R-hiKNGyJd"
      },
      "source": [
        "# 2. 文字起こしデータの活用"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 講義内容をソースとして活用 (RAG導入)\n",
        "\n",
        "モデルの回答の事実性を向上させるためにRetrieval Augmented Generation (RAG)技術を導入します：\n",
        "\n",
        "* **知識ソース**: LLM講座第4講における講師の発言内容\n",
        "* **目的**: モデルに「Inference Time Scaling」に関する正確な知識と文脈を提供し、事実に基づいた回答を促す\n",
        "\n",
        "**初期RAG実装（ベーシックアプローチ）**:\n",
        "* **ドキュメント処理**: 音声認識モデル(speech2text)で書き起こした生テキストをそのまま使用\n",
        "* **分割方法**: 「。」（句点）で区切られた文単位でテキストを分割\n",
        "* **検索手法**: シンプルな類似度ベースの検索でクエリに関連する文を抽出\n",
        "* **制約条件**: モデルの入力トークン制限に収まるよう関連文のみを選択"
      ],
      "metadata": {
        "id": "qcZCmKegHyrl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "47GvcceyObAl"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "emb_model = SentenceTransformer(\"infly/inf-retriever-v1-1.5b\", trust_remote_code=True)\n",
        "# In case you want to reduce the maximum length:\n",
        "emb_model.max_seq_length = 4096"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kPwggQfUS5yl"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/双子素数_本文.txt\", \"r\") as f:\n",
        "  raw_writedown = f.read().replace(\"\\n\", \"\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kxzKF6L2THIw",
        "outputId": "8c597379-6128-478b-edc9-2862a822dcd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ドキュメントサイズ:  61\n",
            "ドキュメントの例: \n",
            " ['数学の未解決問題双子素数は無数に存在するか', '双子素数（ふたごそすう、英:twin prime）とは、差が 2 である二つの素数の組を構成する各素数のことである', '双子素数の組は、(2, 3) を除いた、最も近い素数の組である', '双子素数を小さい順に並べた列は、次の通りである', '(3,5), (5,7), (11,13), (17,19), (29,31), …各組の2素数の平均値（中間の偶数）は、次の通りである', '3連続した数 (a, a+1, a+2) は2と3双方の倍数を含むことから、3の倍数で唯一素数である 3 を含む (3, 5) の組である 4 以外は全て 6 (=2x3) の倍数となる', '4,6,12,18,30,42,60,72,102,108,138, …双子素数の予想[編集]数学上の未解決問題双子素数は無限に存在するか', '素数が無数に存在することは古代ギリシアで既に知られており、ユークリッドの『原論』に証明がある', 'これに対し、双子素数が無数に存在するかという問題、いわゆる「双子素数の予想」は、いまだに数学上の未解決問題である', '双子素数予想が古代ギリシア時代から知られていたとの記述も一部文献に見られるが、確証は得られていない', '(A. de Polignac(1849)) は、双子素数予想を一般化して、任意の偶数を差とする素数の組が無数にあるか、という問題を提出している', '上からの評価式など部分的な結果があるが、その中でも漸近公式の予想は注目に値する', '双子素数の組の数の漸近公式はハーディ・リトルウッド予想の一部であり、これは素数定理と似通った次のような双子素数の漸近的な分布公式を予想している', 'x以下の双子素数の組の数は、漸近的に2Cx(log\\u2061x)2{\\\\displaystyle 2C{\\\\frac {x}{(\\\\log x)^{2}}}}、あるいは2C∫2xdx(log\\u2061x)2{\\\\displaystyle 2C\\\\int _{2}^{x}{\\\\frac {dx}{(\\\\log x)^{2}}}}で与えられる', '後者の積分による表示式の方が良い近似を与える', 'ここで、定数Cは次のような無限積で定義される', 'C=∏p>2{1−1(p−1)2}=0.6601⋯{\\\\displaystyle C=\\\\prod _{p>2}\\\\left\\\\{1-{\\\\frac {1}{(p-1)^{2}}}\\\\right\\\\}=0.6601\\\\cdots }この定数Cは「ハーディ・リトルウッド定数」の一つである', 'この問題は、特に2素数の場合のゴールドバッハの予想に密接に関係しており、篩法などの研究者によって双方の研究が同時に進められてきた', '2004年5月に、「双子素数が無数に存在することの証明」と題された論文が Richard Arenstorf によって提出され[1]、上記のハーディ・リトルウッドの予想が正しいと主張されたが、内容に重大な誤りがあるとして著者自身によって撤回された', '例[編集]最初の20組の双子素数[編集](3,5), (5,7), (11,13), (17,19), (29,31), (41,43), (59,61), (71,73), (101,103), (107,109), (137,139), (149,151), (179,181), (191,193), (197,199), (227,229), (239,241), (269,271), (281,283), (311,313), …各組の双子素数の関数[編集]数OEIS小さいほうの数A001359大きいほうの数A006512平均値の偶数A014574和A054735積A037074知られている最大の双子素数[編集]2020年7月現在で知られている最大の双子素数は、388,342 桁の2996863034895 × 21290000± 1である', 'これは、2016年9月に分散コンピューティングプロジェクトの一つであるPrimeGridにより発見された[2]', '双子素数に関する諸結果[編集](3, 5)を除く全ての双子素数は(6n− 1, 6n+ 1)（nは特定の自然数）の形であり、これは(3, 5)を除く双子素数同士の和が、常に12の倍数であることを意味する', '最初の2組を除き、双子素数の一の位は（十進法で）(1, 3), (7, 9), (9, 1)のいずれかである', 'xより小さな双子素数の個数は高々O(x/(log\\u2061x)2){\\\\displaystyle O\\\\left(x/(\\\\log x)^{2}\\\\right)}である', 'したがって、pとp+ 2がともに素数の場合、次式は収束する (Brun, 1919)', 'B2=∑p(1p+1p+2){\\\\displaystyle B_{2}=\\\\sum _{p}\\\\left({\\\\frac {1}{p}}+{\\\\frac {1}{p+2}}\\\\right)}（双子素数の逆数和）この値 (1.90強) をブルン定数と呼ぶ', '素数の逆数和は発散するので、素数の中で双子素数は、さほど多くはないといえる', 'また、すべての偶数は、高々9個の素数の積で表される2つの整数の差として無限通りに表すことができることもヴィーゴ・ブルンは示している (Brun, 1920)', 'これらの結果は篩法によるものであり、篩法の最初の本格的な成果である', 'それと同時に、双子素数に関する最初の理論的な結果であり、双子素数に関する研究の出発点となった', 'ブルン定数B2の2005年時点での最も正確な値は、B2= 1.902160583104…である', 'この値は、1016までに現れる双子素数を使用して求められた (Sebah, 2002)', 'なお、1994年にブルン定数を計算する過程で P54CPentiumの浮動小数点演算命令にバグが存在することが発見され、話題となった（詳しくはPentiumを参照）', '陳景潤(Chen Jing Run) は、p+ 2が高々2個の素数の積となるような素数pが無数に存在することを示している (Chen, 1966)', 'p+ 2が高々2個の素数の積となるような素数pを陳素数と定義したとき、無限個の陳素数の3項等差数列が存在する（Ben Green,テレンス・タオ, 2005）', '(n,n+ 2)が双子素数であるための必要十分条件は、4{(n− 1)! + 1} +n≡ 0 (modn(n+ 2))である (Clement, 1949)', '2005年、D. Goldston-J. Pintz-C. Yildirim によって次式が証明された', 'lim\\u2006infn→∞pn+1−pnlog\\u2061pn=0.{\\\\displaystyle \\\\liminf _{n\\\\to \\\\infty }{\\\\frac {p_{n+1}-p_{n}}{\\\\log p_{n}}}=0.}素数間間隔ごとの無限存在証明[編集]2013年4月17日に、ニューハンプシャー大学（英語版）の張益唐(Zhang Yitang) は、「隣り合った素数の隔たりが、7千万以下のものが無数組存在する」こと、言い換えるとlim\\u2006infn→∞(pn+1−pn)<7×107{\\\\displaystyle \\\\liminf _{n\\\\to \\\\infty }(p_{n+1}-p_{n})<7\\\\times 10^{7}}を証明した論文“Bounded Gaps Between Primes” を発表し、Annals of Mathematicsにアクセプトされた[3]', 'なお，張益唐の定理に先行する主要な研究結果の詳細解説がテレンス・タオらによって与えられている[4]', '2013年、張益唐の結果から数か月後、ジェームズ・メイナードとテレンス・タオがそれぞれ独立に、素数をm個含む連続した整数の区間が無数に存在する条件を解明した', '区間の幅はmに依存する', '例としてm=2である場合、連続した整数を600ごとに区切ると素数が2個含まれる場合が無数にあり[5]、m=3とすると、素数を3個含み39万5122の幅を持つ区間が無数に存在する', 'これは張益唐の「7000万ごと」を大幅に小さくする成果である[6][7][8]', 'メイナードはこの発見を含む業績により2022年のフィールズ賞を受賞した[9]', '2014年12月現在、張益唐が与えた7千万という間隔（上記のm=2である場合の区間の幅）は246まで狭められている', 'すなわち、間隔が246以内である素数の組は無数に存在する[10]', '脚注[編集][脚注の使い方]^Proof of Infinitely many Twin Primes^PrimePages,The Prime Database: 2996863034895*2^1290000-1^Bounded gaps between primes|Annals of Mathematics^Tao, Terence (2013-06-03),The prime tuples conjecture, sieve theory, and the work of Goldston-Pintz-Yildirim, Motohashi-Pintz, and Zhang,https://terrytao.wordpress.com/2013/06/03/the-prime-tuples-conjecture-sieve-theory-and-the-work-of-goldston-pintz-yildirim-motohashi-pintz-and-zhang/2016年1月14日閲覧', '^別の表現をすると「素数が2個含まれる連続した600の整数の組の最大値は存在しない」^“素数の間隔で新定理発見\\u3000極端な偏りなく分布、米英数学者”. 千葉日報オンライン. (2014年2月26日).https://www.chibanippo.co.jp/newspack/20140226/1812012022年3月19日閲覧', '^“素数の新定理発見\\u3000極端な偏りなく分布\\u3000米英数学者「夢のような成果」”.スポーツニッポン. (2014年2月26日).オリジナルの2015年10月5日時点におけるアーカイブ', '.https://web.archive.org/web/20151005203703/http://www.sponichi.co.jp:80/society/news/2014/02/26/kiji/K20140226007668140.html2024年3月13日閲覧', '{{cite news}}:|work=、|newspaper=引数が重複しています', '(説明)⚠^“素数の間隔で新定理発見\\u3000極端な偏りなく分布、米英数学者”.47NEWS.共同通信社. (2014年2月26日).オリジナルの2014年3月2日時点におけるアーカイブ', '.https://web.archive.org/web/20140302012802/http://www.47news.jp/CN/201402/CN2014022601001180.html2024年3月13日閲覧', '^国際数学連合(2022-07-05),Fields Medals 2022, 国際数学連合,https://www.mathunion.org/imu-awards/fields-medal/fields-medals-20222022年7月5日閲覧', '^Klarreich, Erica (2014-12-10),Prime Gap Grows After Decades-Long Lull, QUANTA magazine,https://www.quantamagazine.org/20141210-prime-gap-grows-after-decades-long-lull/2016年1月14日閲覧', \"参考文献[編集]de Polignac, Alphonse (1849).“Six propositions arithmologiques déduites du crible d'Ératosthène”.Nouvelles annales de mathématiques\\xa0: journal des candidats aux écoles polytechnique et normale8:  423-429.http://www.numdam.org/item/NAM_1849_1_8__423_1/.BRUN, V. (1920).“Le crible d'Eratosthene et le theoreme de Goldbach”.Videnskaps. Skr., Mat. Natur. Kl. Kristiana(3).https://searchworks.stanford.edu/view/9591249.JING-RUN CHEN (1973).“ON THE REPRESENTATION OF A LARGER EVEN INTEGER AS THE SUM OF A PRIME AND THE PRODUCT OF AT MOST TWO PRIMES”.Scientia Sinica16(2):  157-176.doi:10.1360/ya1973-16-2-157.https://doi.org/10.1360/ya1973-16-2-157.and II, ibid. 21(1978), 421-430.本橋洋一『解析的整数論』1 (素数分布論)、朝倉書店〈朝倉数学大系\\xa0; 1〉、2009年\", '国立国会図書館書誌ID:000010611029', 'https://ndlsearch.ndl.go.jp/books/R100000002-I000010611029', '「第2刷 2012：加筆含む」本橋洋一「‘篩法’概観」『数学』第57巻第2号、日本数学会、2005年、138-163頁、doi:10.11429/sugaku1947.57.138', '本橋洋一「素数の翼に乗って」（PDF）『数学通信』第10巻第1号、東京\\xa0: 日本数学会、2005年5月、4-19頁、CRID1520572358126328192、ISSN13421387', 'H. Davenport, Multiplicative Number Theory, 3rd edition, Springer-Verlag, 2002.H. Halberstam and H. E. Richert, Sieve Methods, Academic Press, 1974.M. B. Nathanson, Additive Number Theory: The Classical Bases, Springer-Verlag, 1996.P. Sebar, Counting twin primes and Brun\\'s constant new computation, NMBRTHRY@listserv.nodak.edu mailing list, 2002関連項目[編集]三つ子素数四つ子素数いとこ素数（素数の差が4）セクシー素数（素数の差が6）Prime k-tuple素数定理ハーディ・リトルウッド予想階乗合同算術ブルンの定理外部リンク[編集]『双子素数にまつわるいくつかの話題』 -高校数学の美しい物語Weisstein, Eric W.\"Twin Primes\".mathworld.wolfram.com(英語).Prime Pages,Top-20 Twin Primes表話編歴素数の分類生成式フェルマー(22n+ 1)メルセンヌ(2p− 1)二重メルセンヌ(22p−1− 1)ワグスタッフ((2p+ 1)/3)プロス(k·2n+ 1)階乗(n! ± 1)素数階乗(pn# ± 1)ユークリッド(pn# + 1)ピタゴラス(4n+ 1)ピアポント(2u·3v+ 1)四次(x4+y4)ソリナス(2a± 2b± 1)カレン(n·2n+ 1)ウッダル(n·2n− 1)Cuban((x3−y3)/(x−y))キャロル((2n− 1)2− 2)Kynea((2n+ 1)2− 2)レイランド(xy+yx)サービト(3·2n− 1)ミルズ([A]3n)漸化式フィボナッチリュカペルニューマン–シャンクス–ウィリアムズペラン分割ベルモツキン各種の性質ヴィーフェリッヒ（英語版）(対（英語版）)ウォール–孫–孫（英語版）ウォルステンホルムウィルソン幸運フォーチュンラマヌジャン（英語版）ピライ正則強（英語版）スターンSupersingular (楕円曲線)（英語版）Supersingular (ムーンシャイン理論)（英語版）良いスーパーヒッグス（英語版）高度コトーティエント（英語版）基数依存ハッピー二面体回文エマープレピュニット((10n− 1)/9)置換可能循環切り捨て可能ストロボグラマティック素数Minimal（英語版）弱いフルサイクルプライムUnique（英語版）Primeval（英語版）自己スマランダチェ–ウェラン（英語版）組互いに素双子(p,p+ 2)Bi-twin chain(n− 1,n+ 1, 2n− 1, 2n+ 1, …)三つ子(p,p+ 2orp+ 4,p+ 6)四つ子(p,p+ 2,p+ 6,p+ 8)k−Tupleいとこ(p,p+ 4)セクシー(p,p+ 6)陳ソフィー・ジェルマン(p, 2p+ 1)カニンガム鎖(p, 2p± 1, …)安全(p, (p− 1)/2)算術数列（英語版）(p+an;n= 0, 1, …)平衡(p−n,p,p+n)桁数タイタニック(103桁以上)巨大(104桁以上)メガ(106桁以上)複素数アイゼンシュタイン素数（英語版）ガウス素数合成数擬素数概素数半素数楔数Interprime（英語版）関連する話題確率的素数Industrial-grade prime（英語版）違法素数素数の公式（英語版）素数の間隔『巨大な素数の一覧』最初の50個2357111317192329313741434753596167717379838997101103107109113127131137139149151157163167173179181191193197199211223227229素数の一覧表話編歴素数に関する予想ハーディ・リトルウッド予想1st2ndAgoh–GiugaアンドリカArtin\\'sBateman–Hornブロカールブニャコフスキーen:Chinese hypothesisCramér\\'sDickson\\'sElliott–HalberstamFiroozbakht\\'sGilbreath\\'sGrimm\\'sen:Landau\\'s problemsゴールドバッハ弱ルジャンドル双子素数ルジャンドル予想Lemoine\\'sメルセンヌOppermann\\'sPolignac\\'sポリアen:Schinzel\\'s hypothesis HWaring\\'s prime number「https://ja.wikipedia.org/w/index.php?title=双子素数&oldid=102430056」から取得']\n"
          ]
        }
      ],
      "source": [
        "# ドキュメントを用意する。\n",
        "documents = [text.strip() for text in raw_writedown.split(\"。\")]\n",
        "print(\"ドキュメントサイズ: \", len(documents))\n",
        "print(\"ドキュメントの例: \\n\", documents)#[len(documents)-1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"from bs4 import BeautifulSoup\n",
        "\n",
        "# ファイルのパス\n",
        "file_path = \"/content/事象の地平面 - Wikipedia.html\"\n",
        "\n",
        "# HTMLを読み込んでパース\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    soup = BeautifulSoup(f, \"html.parser\")\n",
        "\n",
        "# 本文に相当する要素（Wikipediaの主要な内容部分）\n",
        "content_div = soup.find(id=\"mw-content-text\")\n",
        "\n",
        "# 本文テキストを抽出\n",
        "text_content = content_div.get_text(separator=\"\\n\", strip=True) if content_div else \"\"\n",
        "\n",
        "# テキストファイルとして保存\n",
        "output_path = \"/content/事象の地平面_本文.txt\"\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(text_content)\n",
        "\n",
        "output_path\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "w9Q_vtFFaNlF",
        "outputId": "7d5965f7-b789-4ae2-b22c-4f795232aa7c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from bs4 import BeautifulSoup\\n\\n# ファイルのパス\\nfile_path = \"/content/事象の地平面 - Wikipedia.html\"\\n\\n# HTMLを読み込んでパース\\nwith open(file_path, \"r\", encoding=\"utf-8\") as f:\\n    soup = BeautifulSoup(f, \"html.parser\")\\n\\n# 本文に相当する要素（Wikipediaの主要な内容部分）\\ncontent_div = soup.find(id=\"mw-content-text\")\\n\\n# 本文テキストを抽出\\ntext_content = content_div.get_text(separator=\"\\n\", strip=True) if content_div else \"\"\\n\\n# テキストファイルとして保存\\noutput_path = \"/content/事象の地平面_本文.txt\"\\nwith open(output_path, \"w\", encoding=\"utf-8\") as f:\\n    f.write(text_content)\\n\\noutput_path\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "nK4cYURzTHIx",
        "outputId": "961570a4-c822-4236-eb31-fcc8a2da8ead",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 200781 has 14.74 GiB memory in use. Of the allocated memory 14.36 GiB is allocated by PyTorch, and 262.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-7ce3fb90c839>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mquery_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdocument_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 各ドキュメントの類似度スコア\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m                 \u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"hpu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m                     \u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m             \u001b[0mmodule_kwarg_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0mmodule_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule_kwarg_keys\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    440\u001b[0m         }\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0moutput_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrans_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m         \u001b[0moutput_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/infly/inf-retriever-v1-1.5b/5b943ba0a743f725bda9577c03b1bc0d13e18d02/modeling_qwen.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, labels, is_causal)\u001b[0m\n\u001b[1;32m   1036\u001b[0m                 )\n\u001b[1;32m   1037\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m                 attention_mask = _prepare_4d_attention_mask_for_sdpa(\n\u001b[0m\u001b[1;32m   1039\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_attn_mask_utils.py\u001b[0m in \u001b[0;36m_prepare_4d_attention_mask_for_sdpa\u001b[0;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mAttentionMaskConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expand_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_attn_mask_utils.py\u001b[0m in \u001b[0;36m_expand_mask\u001b[0;34m(mask, dtype, tgt_len)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mexpanded_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0minverted_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mexpanded_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minverted_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverted_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1071\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rsub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1073\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 216.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 200781 has 14.74 GiB memory in use. Of the allocated memory 14.36 GiB is allocated by PyTorch, and 262.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# Retrievalの実行\n",
        "question = \"数学における双子素数とは何ですか?\"\n",
        "\n",
        "query_embeddings = emb_model.encode([question], prompt_name=\"query\")\n",
        "document_embeddings = emb_model.encode(documents)\n",
        "\n",
        "# 各ドキュメントの類似度スコア\n",
        "scores = (query_embeddings @ document_embeddings.T) * 100\n",
        "print(scores.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_v8gx_tTHIx"
      },
      "outputs": [],
      "source": [
        "topk = 5\n",
        "for i, index in enumerate(scores.argsort()[0][::-1][:topk]):\n",
        "  print(f\"取得したドキュメント{i+1}: (Score: {scores[0][index]})\")\n",
        "  print(documents[index], \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ow0wZy6ETHIx"
      },
      "outputs": [],
      "source": [
        "references = \"\\n\".join([\"* \" + documents[i] for i in scores.argsort()[0][::-1][:topk]])\n",
        "query =  f\"[参考資料]\\n{references}\\n\\n[質問] 数学における双子素数とは何ですか?\"\n",
        "response = generate_output(query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # 評価\n",
        "# score = evaluate_answer_accuracy(question, response, gold_answer)\n",
        "# print(score)"
      ],
      "metadata": {
        "id": "XcXCKsyyHWxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn7tih0RTTzr"
      },
      "source": [
        "### 結果 (初期RAG実装)\n",
        "\n",
        "講義内容のドキュメントを追加したにもかかわらず、モデルの回答には依然として以下の問題が見られます：\n",
        "* 「高速に推論する」など、従来の一般的な推論最適化と「Inference Time Scaling」を混同した誤った解釈が継続\n",
        "* 講義内容を参照しているものの、概念の本質を正確に捉えられていない\n",
        "\n",
        "### 問題分析\n",
        "以下の要因が考えられます：\n",
        "1. **ドキュメント品質の問題**: 音声認識による文字起こしの精度不足\n",
        "2. **検索精度の課題**: 単純な文単位の分割では文脈が失われ、関連性の高いドキュメント片を適切に取得できていない可能性"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 書き起こしテキストの品質改善\n",
        "\n",
        "日本語の音声認識（speech2text）モデルは精度に課題があることが知られています。以下に「LLMにおけるInference Time Scalingとは？」に関連する講義内容の書き起こしテキストを比較します："
      ],
      "metadata": {
        "id": "d-lKuaCSVp_s"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q83QyfAIphk6"
      },
      "source": [
        "### 講義中の該当発言 (LLM講座Day4後半から抜粋)\n",
        "---\n",
        "\n",
        "<修正前>\n",
        "---\n",
        "\n",
        "講義に戻ります。ちょっと練習の時間もあるのであと20分ぐらいで駆け足になりますけど、最後最近のスケールトレンドって話で**生のGENIACLM**の話をして終わろうと思いですねちょっとモチベーションから話すと、ちょっと頭で考えてみてほしいとか見れば一瞬で思うとんですけどバナナの色は何ですかって言われたときと、今日の講義聞いた上で、**ゲームソフトの問題は何だと思いますか**って聞かれたとき、多分あの考えることが違うと思うんですね。**羽の色なんですか**っていうと一瞬黄色ですねもしかしたら緑かもしれないけどぐらいですかね物によるかなみたいなおもちゃだったら違うかもみたいな、だんだんあの、考えていくといろいろ出てくるかもしれないすけど、少なくとも**スケール足の問題なんだと思いますか**って聞かれたときに、今日の話からするとスケール則っていうのはこういうものだからどうだろうこの辺が問題かなみたいな考えとやっぱ思考としては違うってことは何となく思うかなと思います。なんか人間的にはこの二つって全然違うしあの、答えるのに必要な考え方っていうのも違うように思えるわけです。**スケールって言ってる7Gのスケール**って言ってるのはこういった形で、あの簡単なものについては簡単に答えてもいいですし、そうじゃなくて、あの考えなきゃいけない問題に対しては、考える時間を、に計算式を使うというふうにしたときに、これいいことがあるのかっていうような話になってます。二つで、ちょっと順番が前後しますけどこれの仕組みは言語モデルでも効果的ですかっていう話と、これをどう実現できるかっていう、こういう二つの話が最近のトレンドとして出てきています。効果的ですかっていうのが、最近**大湾**と呼ばれる論文が論文じゃないか、モデルが**オペル**から出ましたプレビューとして出てますけどこの法案で注目されていますこれあの**論文にROMってかブログ**にあるとイエスって右側が訓練時の計算資源をスケールさせたときに、初めて何かロジックのベンチマークがあるんですけどこれをがどうなったかで何となくスケールしてると右側がテストTimeコンピュートっていうふうに書いてると思うんすけど、**水温時**に計算資源を増やしたときあるモデルを使うんだけど、簡単に答える方法と深く考えて答える方法みたいでだんだんコース計算式を増やしていったときに、性能がどう変わるかっていうのでこれもスケールしていってるということがわかると思います。こういった形で、要は考える時間をどうやら推論時に使うと計算資源を推論使うのはいいことがありそうだということがわかります。\n",
        "\n",
        "---\n",
        "<修正後>\n",
        "---\n",
        "\n",
        "講義に戻ります。ちょっと演習の時間もあるのであと20分ぐらいで駆け足になりますけど、最後最近のスケールトレンドってことで**「推論時のスケーリング」**についての話をして終わろうと思います。モチベーションから話すと、ちょっと頭で考えてみてもらえれば一瞬でわかると思うとんですけど、「バナナの色は何ですかって言われたとき」と、今日の講義聞いた上で、**「スケール則の問題は何だと思いますか」**って聞かれたとき、多分あの考えることが違うと思うんですね。\n",
        "**「バナナの色なんですか」**っていうと黄色ですね。もしかしたら緑かもしれないけど、物によるかなみたいな、おもちゃだったら違うかもみたいな、だんだんあの、考えていくといろいろ出てくるかもしれないすけど、少なくとも**「スケール則の問題なんだと思いますか」**って聞かれたときに、今日の話からするとスケール則っていうのはこういうものだから「どうだろう」「この辺が問題かな」みたいな考えとはやっぱ思考としては違うってことは何となく思うかなと思います。\n",
        "なんか人間的にはこの二つって全然違うしあの、答えるのに必要な考え方っていうのも違うように思えるわけです。**推論時のスケールって言ってるのは**こういった形で、あの簡単なものについては簡単に答えてもいいですし、そうじゃなくて、深く考えなきゃいけない問題に対しては、考える時間に計算資源を使うというふうにしたときに、これいいことがあるのかっていうような話になってます。\n",
        "これの仕組みは言語モデルでも効果的ですかっていう話と、これをどう実現できるかっていう、こういう二つの話が最近のトレンドとして出てきています。効果的ですかっていうのが、最近**o1**と呼ばれるモデルが**OpenAI**から出ました。プレビューとして出てますけどこのo1で注目されています。これあのo1の**論文ってかブログ**にある図で、左側が訓練時の計算資源をスケールさせたときに、AIMEというロジックのベンチマークがあるんですけど、accuracyがどうなったかというと、何となくスケールしてる。右側がtest-time computeっていうふうに書いてると思うんすけど、**推論時**に計算資源を増やしたときあるモデルを使うんだけど、簡単に答える方法と深く考えて答える方法みたいでだんだん計算資源を増やしていったときに、性能がどう変わるかっていうので、これもスケールしていってるということがわかると思います。\n",
        "こういった形で、要は考える時間をどうやら推論時に使うと、つまり計算資源を推論時に使うのはいいことがありそうだということがわかります。\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCrp81WzyhYc"
      },
      "source": [
        "---\n",
        "### 文字起こしの誤り\n",
        "\n",
        "上記の比較からわかるように、音声認識による書き起こしには重大な誤りが多数含まれています：\n",
        "* 「スケール則の問題」→「ゲームソフトの問題」\n",
        "* 「o1」→「大湾」\n",
        "といった明らかに文脈に合わない単語変換が発生しています。\n",
        "\n",
        "`LLM2024_day4_raw.txt`の中には、このような誤変換が多数見られます。これらの誤りはRAG性能に直接影響し、モデルの回答精度を低下させる要因となります。\n",
        "\n",
        "したがって、**ドキュメント品質の改善**を行い、RAG性能の向上を図ります。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 講義内容をソースとして活用：改善版RAG実装\n",
        "\n",
        "* **ドキュメント処理**:\n",
        "  - speech2textによる書き起こしテキストを人手で丁寧に修正\n",
        "  - 専門用語（Inference Time Scaling、GPT-o1など）の正確な表記を確保\n",
        "  - 文脈の流れを維持しつつ、文法的に正確な日本語に修正\n",
        "\n",
        "* **検索手法**:\n",
        "  - 引き続き「。」（句点）で区切られた文単位でテキストを分割\n",
        "  - 文単位の検索により、モデルの入力トークン制限内で関連情報を最大化\n",
        "\n",
        "この改善により、モデルが正確な情報に基づいて「Inference Time Scaling」の概念を理解し、適切な回答を生成することが期待されます。"
      ],
      "metadata": {
        "id": "T8VWQ25MII7h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNjIC4RnzkNW"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/lecture-ai-engineering/day3/data/LLM2024_day4.txt\", \"r\") as f:\n",
        "  raw_writedown = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f53OojeTzkNW"
      },
      "outputs": [],
      "source": [
        "# ドキュメントを用意する。\n",
        "documents = [text.strip() for text in raw_writedown.split(\"。\")]\n",
        "print(\"ドキュメントサイズ: \", len(documents))\n",
        "print(\"ドキュメントの例: \\n\", documents[310])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlduigQ3OfoN"
      },
      "outputs": [],
      "source": [
        "# Retrievalの実行\n",
        "question = \"LLMにおけるInference Time Scalingとは？\"\n",
        "\n",
        "query_embeddings = emb_model.encode([question], prompt_name=\"query\")\n",
        "document_embeddings = emb_model.encode(documents)\n",
        "\n",
        "# 各ドキュメントの類似度スコア\n",
        "scores = (query_embeddings @ document_embeddings.T) * 100\n",
        "print(scores.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNsGUsnlOoMm"
      },
      "outputs": [],
      "source": [
        "topk = 5\n",
        "for i, index in enumerate(scores.argsort()[0][::-1][:topk]):\n",
        "  print(f\"取得したドキュメント{i+1}: (Score: {scores[0][index]})\")\n",
        "  print(documents[index], \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoOCvFW4ltcA"
      },
      "outputs": [],
      "source": [
        " #回答に役立つ該当の発言はreference[1871]〜に含まれてます。\n",
        "references = \"\\n\".join([\"* \" + documents[i] for i in scores.argsort()[0][::-1][:topk]])\n",
        "query =  f\"[参考資料]\\n{references}\\n\\n[質問] LLMにおけるInference Time Scalingとは？\"\n",
        "response = generate_output(query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FbzMLfTtWxx"
      },
      "outputs": [],
      "source": [
        "# # 評価\n",
        "# score = evaluate_answer_accuracy(question, response, gold_answer)\n",
        "# print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLe0IJPeH97d"
      },
      "source": [
        "## 結果 (修正テキストによるRAG)\n",
        "\n",
        "書き起こしテキストの品質改善により、モデルの回答に部分的な向上が見られました：\n",
        "\n",
        "### 改善点\n",
        "* 「推論時に計算資源を増やすこと」という概念を正確に捉えるようになった\n",
        "\n",
        "### 問題点\n",
        "* 「Pretraining時」という記述は講義内容と矛盾している\n",
        "\n",
        "### 問題分析\n",
        "\n",
        "モデルが誤った回答を生成する主要因として、**文脈の欠如**が考えられます：\n",
        "* 「。」で区切られた短い文単位での検索では、各文の発言背景や関連性が失われる\n",
        "* 単独の文から情報を抽出するため、講師の全体的な主張や議論の流れを把握できない\n",
        "* 結果として、正しい個別の文でも、その解釈に必要な背景情報が欠如し、誤った文脈で理解される\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 文脈を考慮したチャンク化の導入\n",
        "\n",
        "検索結果の品質向上のため、以下の改善を実施します：\n",
        "\n",
        "* **前後文脈を含むチャンク化**:\n",
        "  - 検索でマッチした文だけでなく、その前後の複数文も含めてチャンクとして取得\n",
        "  - 具体的には、マッチした文を中心に前2文、後2文を含む計5文程度のチャンクを構成\n",
        "  - この「文脈ウィンドウ」により、発言の背景情報や議論の流れが保持される\n",
        "\n",
        "* **期待される効果**:\n",
        "  - 講師の主張とその根拠の関係性を正確に把握できる\n",
        "  - 概念の定義とその適用範囲を正しく理解できる\n",
        "\n",
        "この改善により、モデルが講義内容の本質をより正確に理解し、一貫性のある事実に基づいた回答を生成することが期待されます。"
      ],
      "metadata": {
        "id": "QE3VhpppWejB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94uovDFrVOTJ"
      },
      "outputs": [],
      "source": [
        "# 前後それぞれ2つずつの文章を一つのドキュメントに追加する。（要は5つの文章集合になる)\n",
        "references = \"\\n\".join([\"* \" + \"。\".join(documents[max(0, i-2): min(i+2, len(documents))]).strip() for i in scores.argsort()[0][::-1][:topk]])\n",
        "query =  f\"[参考資料]\\n{references}\\n\\n[質問] LLMにおけるInference Time Scalingとは？\"\n",
        "response = generate_output(query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAzYsxVWVdMS"
      },
      "outputs": [],
      "source": [
        "# # 評価\n",
        "# score = evaluate_answer_accuracy(question, response, gold_answer)\n",
        "# print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD3R54G1WX8B"
      },
      "source": [
        "## 結果 (文脈付きチャンク化によるRAG)\n",
        "\n",
        "文脈を含むチャンク化により、モデルの回答の方向性に明確な改善が見られました：\n",
        "\n",
        "### 改善点\n",
        "* 「推論時の計算をスケールさせる」という概念を据えて回答\n",
        "* Inference Time Scalingの基本原理についての理解が向上\n",
        "\n",
        "### 残存する問題点\n",
        "* 質問と関連性の低い情報（ノイズ）が混入する\n",
        "\n",
        "### 問題分析\n",
        "\n",
        "文脈付きチャンク化によるアプローチで新たに発生した課題：\n",
        "\n",
        "1. **情報過多の問題**:\n",
        "   * ドキュメント量の増加により、モデルに提供される情報総量が大幅に増加\n",
        "   * 関連情報と非関連情報が混在し、ノイズと重要情報の区別が困難に\n",
        "\n",
        "2. **情報選択の複雑化**:\n",
        "   * モデルは単に回答を生成するだけでなく、提供された多様な情報源から関連性の高い情報を選別する作業も担うことになった\n",
        "   * この二重タスクにより回答生成の難易度が上昇\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Rerankによる情報品質の向上\n",
        "\n",
        "検索精度をさらに向上させるため、二段階の検索プロセスを導入します：\n",
        "\n",
        "* **Rerank手法の導入**:\n",
        "  - 第一段階: 従来通り基本的な検索アルゴリズムでtop-k個のドキュメントチャンクを取得\n",
        "  - 第二段階: 取得したチャンクに対してLLMを活用した高度な関連性評価を実施\n",
        "  - LLMに「このドキュメントは質問『LLMにおけるInference Time Scalingとは？』に対して本当に関連性が高いか」を判断させる\n",
        "  - 関連性スコアに基づいてランク付けし、真に関連性の高いチャンクのみを選出\n",
        "\n",
        "* **期待される効果**:\n",
        "  - 質の高い情報に焦点を絞ることで、ノイズとなる情報を大幅に削減\n",
        "  - 文脈を保ちながらも、関連性の高い情報のみをモデルに提供\n",
        "  - モデルのタスクを「多量の情報から選別して回答」から「厳選された情報に基づいて回答」へと単純化\n",
        "\n",
        "この高度な情報フィルタリングにより、Inference Time Scalingに関する正確で一貫性のある回答生成が期待されます。"
      ],
      "metadata": {
        "id": "BP0cUKrUZmOj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HfzJ5EpXGtj"
      },
      "outputs": [],
      "source": [
        " #回答に役立つ該当の発言はreference[1871]〜に含まれてます。\n",
        "references = []\n",
        "for ref in [\"。\".join(documents[max(0, i-2): min(i+2, len(documents))]).strip() for i in scores.argsort()[0][::-1][:topk]]:\n",
        "\n",
        "  query = f\"与えられた[参考資料]が[質問]に直接関連しているかを、'yes''no'で答えること。[参考資料]\\n{ref}\\n\\n[質問] LLMにおけるInference Time Scalingとは？\"\n",
        "  response = generate_output(query)\n",
        "\n",
        "  print(\"\\n\\n対象となるドキュメント:\\n\", ref.replace(\"。\", \"。\\n\"))\n",
        "  print(\"\\n関連しているかどうか: \", response)\n",
        "\n",
        "  if \"yes\" in response.lower():\n",
        "    references.append(ref)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLietDaD5I3h"
      },
      "outputs": [],
      "source": [
        "print(len(references))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs74h4ADXj99"
      },
      "source": [
        "上記より、上位4件のみが関連しているとわかったので、これらだけをモデルに渡すこととする。（生成内容が確立的なので、4件でない可能性もあります）"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fu9wBykZXxja"
      },
      "outputs": [],
      "source": [
        "query =  f\"[参考資料]\\n{references}\\n\\n[質問] LLMにおけるInference Time Scalingとは？\"\n",
        "response = generate_output(query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5kHntvSXxjb"
      },
      "outputs": [],
      "source": [
        "# # 評価\n",
        "# score = evaluate_answer_accuracy(question, response, gold_answer)\n",
        "# print(score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elqD2gJt5RCo"
      },
      "source": [
        "## 結果 (Rerank導入後)\n",
        "\n",
        "Rerankの導入により、回答品質に改善が見られました：\n",
        "\n",
        "### 達成された成果\n",
        "* Inference Time Scalingに関する正確な情報を含んだ回答の生成\n",
        "* 無関係な情報やノイズの排除\n",
        "* 講義内容を反映した説明の実現 🎉\n",
        "\n",
        "この結果から、RAGパイプラインにおける情報の質と関連性の重要性であり、検索で取得した情報を単に増やすだけでなく、その情報の関連性を精査する方法を学ぶことができました。"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. さらなる改善案: 意味的チャンク化\n",
        "\n",
        "文単位での分割と前後文脈の追加という現在のアプローチをさらに発展させる手法として、**意味的なチャンク化**が考えられます：\n",
        "\n",
        "* **意味的チャンク（段落）単位での分割**:\n",
        "  - 単純な文の区切りではなく、意味的なまとまり（トピック、議論、例示など）に基づいてテキストを分割\n",
        "  - 人間の主観に基づく意味的な段落分けを活用\n",
        "  - 各チャンクが「一つの完結した考え」を表現するようにする\n",
        "\n",
        "* **期待される効果**:\n",
        "  - より自然な文脈理解が可能に（人間の思考や会話の流れに近い）\n",
        "  - トピックの開始から結論までの流れを維持できる\n",
        "  - 概念間の関係性や比較が同一チャンク内に含まれ、より深い理解につなげる\n",
        "\n",
        "* **検証方法**:\n",
        "  - 人間が主観的に意味でグループ化したチャンクセットを用意\n",
        "  - 同じRerank手法を適用し、文単位チャンクとの性能差を比較\n",
        "  - 回答の正確性、一貫性、網羅性を評価指標として使用\n",
        "\n",
        "この意味的チャンク化手法は、特に講義のような構造化された発話においては、より自然で効果的な情報検索と理解を可能にすると予想されます。"
      ],
      "metadata": {
        "id": "QXCBf_ixJ3jY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU_ttvcNKayo"
      },
      "source": [
        "**注意事項**\n",
        "\n",
        "**ここから先のセルを実行した場合、GPUメモリ不足になる可能性が高いです。**\n",
        "\n",
        "\n",
        "このノートブックでは、GPUはT4を使用しています。\n",
        "Colab Pro等を契約し、L4などのよりGPUメモリの大きいものを使用するか、モデルやその設定等を変更するなどの工夫が必要になります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4DNOyPNXAtl3"
      },
      "outputs": [],
      "source": [
        "# 本来は段落をそのままdocumentsに入れずに一定のサイズに分割した方が良いでしょうが、簡単のために段落をそのまま入れてしまいます。\n",
        "documents = [text.replace(\"\\n\", \" \").strip() for text in raw_writedown.split(\"\\n\\n\")]\n",
        "print(\"ドキュメントサイズ: \", len(documents))\n",
        "print(\"ドキュメントの例: \\n\", documents[30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF6wc10RAxuc"
      },
      "outputs": [],
      "source": [
        "question = \"LLMにおけるInference Time Scalingとは？\"\n",
        "\n",
        "query_embeddings = emb_model.encode([question], prompt_name=\"query\")\n",
        "document_embeddings = emb_model.encode(documents)\n",
        "\n",
        "scores = (query_embeddings @ document_embeddings.T) * 100\n",
        "print(scores.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-FKkAcTA-Sx"
      },
      "outputs": [],
      "source": [
        "# 簡単のためにtop2でやります。結果を見てもらえれば問題なく関連する項目のみ取得できているのが分かるかと思います。\n",
        "topk = 2\n",
        "for i, index in enumerate(scores.argsort()[0][::-1][:topk]):\n",
        "  print(f\"取得したドキュメント{i+1}: (Score: {scores[0][index]})\")\n",
        "  print(documents[index], \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtC-wsj4BGwn"
      },
      "outputs": [],
      "source": [
        "reference = \"\\n\".join([\"* \" + documents[i] for i in scores.argsort()[0][::-1][:topk]])\n",
        "query =  f\"[参考資料]\\n{references}\\n\\n[質問] LLMにおけるInference Time Scalingとは？\"\n",
        "response = generate_output(query)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJVL3u6lCc8k"
      },
      "outputs": [],
      "source": [
        "# # 評価\n",
        "# score = evaluate_answer_accuracy(question, response, gold_answer)\n",
        "# print(score)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "95fa6bb538884010bbc66884852df506": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_72e8723090d14f14b167e911599beec9"
          }
        },
        "1ea9b696446b4bd584ea886051b5d982": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18b43a21a8524ebb954092d8df3c630e",
            "placeholder": "​",
            "style": "IPY_MODEL_ea2ae871a67441c1891c319ef6497ecc",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "3b3cab67461347808789838cfc744e1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_c157587fd8c0406a88255fc79de2e099",
            "placeholder": "​",
            "style": "IPY_MODEL_9c525cb180c4425398a88aec3860f368",
            "value": ""
          }
        },
        "db4b5a26d62943bb84f83bf6f272d18a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_cd8dfa238fd9450a94f3e663ad861d02",
            "style": "IPY_MODEL_fea17e9a044945d8bf91bd8d5cfbbd05",
            "value": true
          }
        },
        "32db503f37af44acbc6decbfa68c335e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_23914dd39cb44ca4a77e4c5617842650",
            "style": "IPY_MODEL_5bf285b1cedb495a9776d73efba7007b",
            "tooltip": ""
          }
        },
        "8e168a6efe504814a70326992590045c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b71e366ebdf941ad849bd1085e8b962d",
            "placeholder": "​",
            "style": "IPY_MODEL_cdb2fd205a4545bea4eca89022e3738e",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "72e8723090d14f14b167e911599beec9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "18b43a21a8524ebb954092d8df3c630e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea2ae871a67441c1891c319ef6497ecc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c157587fd8c0406a88255fc79de2e099": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c525cb180c4425398a88aec3860f368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd8dfa238fd9450a94f3e663ad861d02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fea17e9a044945d8bf91bd8d5cfbbd05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23914dd39cb44ca4a77e4c5617842650": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bf285b1cedb495a9776d73efba7007b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "b71e366ebdf941ad849bd1085e8b962d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdb2fd205a4545bea4eca89022e3738e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1aafe1c212364895a8dac33bac0d7766": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_263049ac05fd439f9dce4c84cc454022",
            "placeholder": "​",
            "style": "IPY_MODEL_299e5ff12ff8494db022e3b4a359b833",
            "value": "Connecting..."
          }
        },
        "263049ac05fd439f9dce4c84cc454022": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "299e5ff12ff8494db022e3b4a359b833": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}